{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torchvision\n",
    "import torch.nn as nn \n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transform = transforms.Compose([\n",
    "    transforms.Resize(size=(256, 256)),\n",
    "    transforms.Grayscale(1),\n",
    "    transforms.RandomHorizontalFlip(p = 0.5),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path('###########')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = path / 'train'\n",
    "test_dir = path /'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir, test_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainFolder = ImageFolder(train_dir, data_transform, target_transform=None)\n",
    "testFolder = ImageFolder(test_dir, data_transform, target_transform=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(trainFolder), len(testFolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_id = trainFolder.class_to_idx\n",
    "class_dict = trainFolder.classes\n",
    "\n",
    "class_id, class_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData = DataLoader(trainFolder, shuffle=True, num_workers= os.cpu_count(), batch_size=4)\n",
    "testData = DataLoader(testFolder, num_workers= os.cpu_count(), batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceModel(nn.Module):\n",
    "    def __init__(self, inshape = 1, hidden_shape = 16, out_shape = 2):\n",
    "        super(FaceModel, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(inshape, hidden_shape, kernel_size=2, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=1),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(hidden_shape, hidden_shape, kernel_size=2, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=1),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(hidden_shape * 256 * 256, out_shape)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FaceModel(inshape=1, hidden_shape=16, out_shape=2).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, label = next(iter(trainData))\n",
    "plt.title(label=label[0])\n",
    "plt.imshow(img[0].permute(1,2,0))\n",
    "plt.plot()\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model(img.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optim = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "acc = Accuracy('BINARY', num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    for batch in tqdm(trainData):\n",
    "        X,y = batch\n",
    "        y_pred = model(X.to(device))\n",
    "        optim.zero_grad()\n",
    "        loss = loss_fn(y_pred.to(device), y.to(device))\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        total_train_loss += loss.item()\n",
    "    total_train_loss /= len(trainData)\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    test_acc = 0\n",
    "    with torch.inference_mode():\n",
    "        for batch in tqdm(testData):\n",
    "            X, y = batch\n",
    "            y_pred = model(X.to(device))\n",
    "            loss = loss_fn(y_pred.to(device), y.to(device))\n",
    "            test_loss += loss.item()\n",
    "            test_acc += acc(y_pred.argmax(dim = 1).to('cpu'), y.to('cpu'))\n",
    "\n",
    "        total_test_loss = test_loss / len(testData)\n",
    "        total_test_Acc = test_acc / len(testData)\n",
    "\n",
    "    print(f'Epoch: {epoch + 1}, trainLoss: {total_train_loss}, test_loss: {total_test_loss}, test Acc: {total_test_Acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'faceRecognition.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Convert BGR frame to RGB\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Convert NumPy array to PIL Image\n",
    "    pil_image = Image.fromarray(rgb_frame)\n",
    "    \n",
    "    # Apply transformations\n",
    "    transformed_image = data_transform(pil_image)\n",
    "    \n",
    "    # Add batch dimension and make a prediction\n",
    "    input_tensor = transformed_image.unsqueeze(0).to(device)  # Add batch dimension and move to device\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(input_tensor)\n",
    "\n",
    "    # Get the predicted class\n",
    "    predicted_class = y_pred.argmax(dim=1).item()\n",
    "    # Convert prediction to string for display\n",
    "    y_pred_str = str(class_dict[predicted_class])\n",
    "    \n",
    "    # Display the prediction on the frame\n",
    "    cv2.putText(frame, y_pred_str, (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "    cv2.imshow('Face Recognition', frame)\n",
    "\n",
    "    # Break the loop if the user presses 'q'\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
